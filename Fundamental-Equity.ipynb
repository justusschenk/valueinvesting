{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "crsp = pd.read_pickle('Data/crsp.pkl')\n",
    "bm = pd.read_csv('Data/Fundamentals/BM.csv')\n",
    "cash_assets = pd.read_csv('Data/Fundamentals/Cash.csv')\n",
    "gross_profitability = pd.read_csv('Data/Fundamentals/GP.csv')\n",
    "mlev = pd.read_csv('Data/Fundamentals/Leverage.csv')\n",
    "poy = pd.read_csv('Data/Fundamentals/PayoutYield.csv')\n",
    "sales_growth = pd.read_csv('Data/Fundamentals/sgr.csv')\n",
    "assets_growth = pd.read_csv('Data/Fundamentals/AssetGrowth.csv')\n",
    "RoE = pd.read_csv('Data/Fundamentals/RoE.csv')\n",
    "am = pd.read_csv('Data/Fundamentals/AM.csv')\n",
    "blev = pd.read_csv('Data/Fundamentals/BookLeverage.csv')\n",
    "asset_turnover = pd.read_csv('Data/Fundamentals/AssetTurnover.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm.rename(columns={'BM':'logBM'}, inplace=True)\n",
    "bm.rename(columns={'yyyymm':'date'}, inplace=True)\n",
    "cash_assets.rename(columns={'yyyymm':'date'}, inplace=True)\n",
    "gross_profitability.rename(columns={'yyyymm':'date'}, inplace=True)\n",
    "mlev.rename(columns={'yyyymm':'date'}, inplace=True)\n",
    "poy.rename(columns={'yyyymm':'date'}, inplace=True)\n",
    "sales_growth.rename(columns={'yyyymm':'date'}, inplace=True)\n",
    "assets_growth.rename(columns={'yyyymm':'date'}, inplace=True)\n",
    "RoE.rename(columns={'yyyymm':'date'}, inplace=True)\n",
    "am.rename(columns={'yyyymm':'date'}, inplace=True)\n",
    "blev.rename(columns={'yyyymm':'date'}, inplace=True)\n",
    "asset_turnover.rename(columns={'yyyymm':'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_yyyymm_to_period(series):\n",
    "    \"\"\"\n",
    "    Converts a Series of dates in yyyymm format to Period[M] objects (i.e., yyyy-mm).\n",
    "    Handles strings or integers, but not full date strings like yyyy-mm-dd.\n",
    "    \"\"\"\n",
    "    series = series.astype(str).str.replace('-', '').str.zfill(6)\n",
    "\n",
    "    # Ensure values are purely digits and 6 characters long\n",
    "    mask = series.str.match(r'^\\d{6}$')\n",
    "    if not mask.all():\n",
    "        raise ValueError(\"Some values in the series are not in yyyymm format.\")\n",
    "\n",
    "    return pd.to_datetime(series + '01', format='%Y%m%d')\n",
    "\n",
    "\n",
    "bm['date'] = convert_yyyymm_to_period(bm['date'])\n",
    "cash_assets['date'] = convert_yyyymm_to_period(cash_assets['date'])\n",
    "gross_profitability['date'] = convert_yyyymm_to_period(gross_profitability['date'])\n",
    "mlev['date'] = convert_yyyymm_to_period(mlev['date'])\n",
    "poy['date'] = convert_yyyymm_to_period(poy['date'])\n",
    "sales_growth['date'] = convert_yyyymm_to_period(sales_growth['date'])\n",
    "assets_growth['date'] = convert_yyyymm_to_period(assets_growth['date'])\n",
    "RoE['date'] = convert_yyyymm_to_period(RoE['date'])\n",
    "am['date'] = convert_yyyymm_to_period(am['date'])\n",
    "blev['date'] = convert_yyyymm_to_period(blev['date'])\n",
    "asset_turnover['date'] = convert_yyyymm_to_period(asset_turnover['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>date</th>\n",
       "      <th>BookLeverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>1987-04-01</td>\n",
       "      <td>5.059808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000</td>\n",
       "      <td>1987-05-01</td>\n",
       "      <td>5.059808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000</td>\n",
       "      <td>1987-06-01</td>\n",
       "      <td>5.059808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>1987-07-01</td>\n",
       "      <td>5.059808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000</td>\n",
       "      <td>1987-08-01</td>\n",
       "      <td>5.059808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3544850</th>\n",
       "      <td>93436</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>1.700040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3544851</th>\n",
       "      <td>93436</td>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>1.700040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3544852</th>\n",
       "      <td>93436</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>1.700040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3544853</th>\n",
       "      <td>93436</td>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>1.700040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3544854</th>\n",
       "      <td>93436</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>1.700040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3544855 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         permno       date  BookLeverage\n",
       "0         10000 1987-04-01      5.059808\n",
       "1         10000 1987-05-01      5.059808\n",
       "2         10000 1987-06-01      5.059808\n",
       "3         10000 1987-07-01      5.059808\n",
       "4         10000 1987-08-01      5.059808\n",
       "...         ...        ...           ...\n",
       "3544850   93436 2025-01-01      1.700040\n",
       "3544851   93436 2025-02-01      1.700040\n",
       "3544852   93436 2025-03-01      1.700040\n",
       "3544853   93436 2025-04-01      1.700040\n",
       "3544854   93436 2025-05-01      1.700040\n",
       "\n",
       "[3544855 rows x 3 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "dfs = [crsp, bm, cash_assets, gross_profitability, mlev, poy, sales_growth, assets_growth, RoE, am, blev, asset_turnover]\n",
    "\n",
    "# Reduce merges all DataFrames on 'permno' and 'date'\n",
    "df = reduce(lambda left, right: pd.merge(left, right, on=['permno', 'date'], how='inner'), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Variables\n",
    "df.rename(columns={'book_equity': 'BE'}, inplace=True)\n",
    "df.rename(columns={'market_equity': 'ME'}, inplace=True)\n",
    "df['payout'] = df['PayoutYield'] * df['ME'].shift(6)\n",
    "df['at'] = df['AM'] * df['ME']\n",
    "\n",
    "#Valuation Measures\n",
    "df.rename(columns={'logBM': 'bm'}, inplace=True)\n",
    "df['poy'] = np.log(1+ df['payout']/df['ME'])\n",
    "df['sales'] = df['AssetTurnover'] * df['at']\n",
    "df['yy'] = np.log(df['sales']/df['ME'])\n",
    "\n",
    "#Growth Measures\n",
    "df['beg'] = np.log(df['BE']/df['BE'].shift(1))\n",
    "df['ag'] = np.log(df['AssetGrowth'])\n",
    "df['yg'] = np.log(df['sgr'])\n",
    "\n",
    "#Profitability Measures\n",
    "df['csprof'] = np.log(1+ (df['payout']+df['BE'].diff())/df['BE'].shift(1))\n",
    "df['roe'] = np.log(df['RoE'])\n",
    "df['gprof'] = np.log(df['GP'])\n",
    "\n",
    "#Capital Structure Measures\n",
    "df.rename(columns={'Leverage': 'mlev'}, inplace=True)\n",
    "df['total_debt'] = (df['mlev']*df['ME'])/(1-df['mlev'])\n",
    "df['blev'] = np.log(df['BookLeverage'])\n",
    "df.rename(columns={'Cash': 'cash'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import fsolve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================\n",
    "# STEP 1: DEFINE ALL VARIABLES\n",
    "# =====================================\n",
    "\n",
    "# You need to provide these as pandas DataFrames with:\n",
    "# - Rows: firm-year observations\n",
    "# - Columns: the variables defined below\n",
    "\n",
    "# === Valuation Measures ===\n",
    "# bm = log book-to-market ratio\n",
    "# poy = payout yield = ln(1 + PO/ME)\n",
    "# yy = sales yield = ln(Y/ME)\n",
    "\n",
    "# === Growth Measures ===\n",
    "# beg = book equity growth = ln(BE_t/BE_t-1)\n",
    "# ag = asset growth = ln(A_t/A_t-1)\n",
    "# yg = sales growth = ln(Y_t/Y_t-1)\n",
    "\n",
    "# === Profitability Measures ===\n",
    "# csprof = clean surplus profitability = ln(1 + (PO + ΔBE)/BE_t-1)\n",
    "# roe = return on equity = ln(1 + E/((BE_t + BE_t-1)/2))\n",
    "# gprof = gross profitability = ln(1 + GP/((A_t + A_t-1)/2))\n",
    "\n",
    "# === Capital Structure Measures ===\n",
    "# mlev = market leverage = B/(ME + B)\n",
    "# blev = book leverage = B/A\n",
    "# cash = cash holdings = C/A\n",
    "\n",
    "# === Other Required Variables ===\n",
    "# BE = book equity\n",
    "# ME = market equity\n",
    "# firm_id = firm identifier\n",
    "# year = year\n",
    "\n",
    "# Example structure (you'll replace with your actual data):\n",
    "# df = pd.DataFrame({\n",
    "#     'firm_id': [...],\n",
    "#     'year': [...],\n",
    "#     'BE': [...],\n",
    "#     'ME': [...],\n",
    "#     'bm': [...],\n",
    "#     'poy': [...],\n",
    "#     'yy': [...],\n",
    "#     'beg': [...],\n",
    "#     'ag': [...],\n",
    "#     'yg': [...],\n",
    "#     'csprof': [...],\n",
    "#     'roe': [...],\n",
    "#     'gprof': [...],\n",
    "#     'mlev': [...],\n",
    "#     'blev': [...],\n",
    "#     'cash': [...]\n",
    "# })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the paper uses annual data, we need to aggregate monthly data\n",
    "# We'll use June of each year (month 6) to match the paper's methodology\n",
    "\n",
    "# Ensure date is datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Extract year and month\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "\n",
    "# Filter to June observations only (or December if that's when fiscal year ends)\n",
    "# You can adjust this based on your fiscal year convention\n",
    "df_annual = df[df['month'] == 6].copy()\n",
    "\n",
    "# Alternatively, if you want to use December values for market equity\n",
    "# but annual accounting data, you might need a more complex merge\n",
    "# For now, we'll use June to match the paper\n",
    "\n",
    "# Sort by permno and year\n",
    "df_annual = df_annual.sort_values(['permno', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# STEP 2: SET PARAMETERS\n",
    "# =====================================\n",
    "\n",
    "# Number of periods for finite sum\n",
    "H = 100\n",
    "\n",
    "# Adjustment parameter for VAR stability\n",
    "theta = 0.8  # Since theta^10 = 0.1\n",
    "\n",
    "# State variable names in order\n",
    "state_vars = ['bm', 'poy', 'yy', 'beg', 'ag', 'yg', \n",
    "              'csprof', 'roe', 'gprof', 'mlev', 'blev', 'cash']\n",
    "\n",
    "# Number of state variables\n",
    "n_vars = len(state_vars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# STEP 3: PREPARE VAR DATA\n",
    "# =====================================\n",
    "\n",
    "# Assuming df is your DataFrame with all the required variables\n",
    "# First, we need to create the state matrix for VAR estimation\n",
    "\n",
    "# Create lagged variables\n",
    "for var in state_vars:\n",
    "    df[f'{var}_lag'] = df.groupby('permno')[var].shift(1)\n",
    "\n",
    "# Remove observations with missing lags\n",
    "df_var = df.dropna(subset=[f'{var}_lag' for var in state_vars])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning and validation...\n",
      "  Removing 5 infinite values from yy\n",
      "  Removing 10 infinite values from ag\n",
      "  Removing 19 infinite values from yg\n",
      "  Removing 4 infinite values from gprof\n",
      "  Removing 10 infinite values from ag_lag\n",
      "  Removing 19 infinite values from yg_lag\n",
      "  Removing 1 infinite values from roe_lag\n",
      "\n",
      "Checking for extreme values:\n",
      "  poy: 17674 extreme values detected\n",
      "  beg: 2 extreme values detected\n",
      "  ag: 24 extreme values detected\n",
      "  yg: 601 extreme values detected\n",
      "  roe: 450343 extreme values detected\n",
      "  mlev: 119626 extreme values detected\n",
      "  blev: 339405 extreme values detected\n",
      "  cash: 322 extreme values detected\n",
      "\n",
      "Estimating VAR parameters with robust methods...\n",
      "\n",
      "Aggregating yearly estimates...\n",
      "\n",
      "Checking and enforcing VAR stability...\n",
      "Initial max eigenvalue: 0.9965\n",
      "VAR is unstable or has numerical issues. Applying shrinkage...\n",
      "After shrinkage, max eigenvalue: 0.9467\n",
      "\n",
      "Final Gamma matrix validation:\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Max absolute value: 0.9966\n",
      "\n",
      "Gamma matrix (first 5x5):\n",
      "[[ 9.22910909e-01 -1.65708440e-03  1.40451113e-02 -6.02184032e-03\n",
      "   1.11374535e-03]\n",
      " [-1.78583694e-04  7.17604006e-01  5.24822015e-04 -2.20893895e-02\n",
      "  -1.10882837e-03]\n",
      " [ 5.55066054e-03  1.89987315e-03  9.35583825e-01  2.91142565e-03\n",
      "   2.09561514e-04]\n",
      " [ 1.06802211e-02 -1.28086553e-01 -7.41373248e-03 -9.32501897e-02\n",
      "  -1.02972350e-03]\n",
      " [-9.38177527e-04 -5.20601423e-04 -9.93064715e-03  2.25325568e-03\n",
      "   8.78393470e-01]]\n",
      "\n",
      "Intercepts (first 5): [-0.02556402  0.00443112 -0.00020537  0.0049582  -0.12223258]\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# STEP 4: ESTIMATE VAR PARAMETERS (ROBUST VERSION)\n",
    "# =====================================\n",
    "\n",
    "# First, let's check and clean the data more thoroughly\n",
    "print(\"Data cleaning and validation...\")\n",
    "\n",
    "# Remove any infinite values\n",
    "for var in state_vars + [f'{var}_lag' for var in state_vars]:\n",
    "    inf_count = np.isinf(df_var[var]).sum()\n",
    "    if inf_count > 0:\n",
    "        print(f\"  Removing {inf_count} infinite values from {var}\")\n",
    "        df_var[var] = df_var[var].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Check for extreme values that might cause numerical issues\n",
    "print(\"\\nChecking for extreme values:\")\n",
    "for var in state_vars:\n",
    "    q99 = df_var[var].quantile(0.99)\n",
    "    q01 = df_var[var].quantile(0.01)\n",
    "    extreme_count = ((df_var[var] > q99 * 10) | (df_var[var] < q01 * 10)).sum()\n",
    "    if extreme_count > 0:\n",
    "        print(f\"  {var}: {extreme_count} extreme values detected\")\n",
    "        # Winsorize extreme values\n",
    "        df_var[var] = df_var[var].clip(lower=df_var[var].quantile(0.001), \n",
    "                                       upper=df_var[var].quantile(0.999))\n",
    "\n",
    "# Initialize storage for VAR parameters\n",
    "Gamma = np.zeros((n_vars, n_vars))\n",
    "intercepts = np.zeros(n_vars)\n",
    "\n",
    "# Use more robust estimation approach\n",
    "print(\"\\nEstimating VAR parameters with robust methods...\")\n",
    "\n",
    "# Method 1: Try year-by-year first with careful handling\n",
    "yearly_estimates = {i: {'intercepts': [], 'coefs': []} for i in range(n_vars)}\n",
    "\n",
    "for year in sorted(df_var['year'].unique()):\n",
    "    year_data = df_var[df_var['year'] == year]\n",
    "    \n",
    "    # Skip if too few observations\n",
    "    if len(year_data) < 2 * n_vars:\n",
    "        continue\n",
    "    \n",
    "    for i, dep_var in enumerate(state_vars):\n",
    "        # Get data for this equation\n",
    "        y = year_data[dep_var].values\n",
    "        X = year_data[[f'{var}_lag' for var in state_vars]].values\n",
    "        \n",
    "        # Create valid data mask\n",
    "        valid_mask = ~np.isnan(y) & ~np.any(np.isnan(X), axis=1)\n",
    "        \n",
    "        if np.sum(valid_mask) < n_vars + 5:\n",
    "            continue\n",
    "        \n",
    "        y_valid = y[valid_mask]\n",
    "        X_valid = X[valid_mask]\n",
    "        \n",
    "        # Standardize variables to improve numerical stability\n",
    "        y_mean, y_std = np.mean(y_valid), np.std(y_valid)\n",
    "        X_mean = np.mean(X_valid, axis=0)\n",
    "        X_std = np.std(X_valid, axis=0)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        y_std = y_std if y_std > 0 else 1\n",
    "        X_std = np.where(X_std > 0, X_std, 1)\n",
    "        \n",
    "        # Standardize\n",
    "        y_standard = (y_valid - y_mean) / y_std\n",
    "        X_standard = (X_valid - X_mean) / X_std\n",
    "        \n",
    "        # Add intercept\n",
    "        X_with_const = np.column_stack([np.ones(len(y_standard)), X_standard])\n",
    "        \n",
    "        try:\n",
    "            # Use least squares with regularization\n",
    "            from scipy.linalg import lstsq\n",
    "            beta_standard, _, _, _ = lstsq(X_with_const, y_standard, \n",
    "                                          lapack_driver='gelsy')\n",
    "            \n",
    "            # Transform back to original scale\n",
    "            beta = np.zeros(n_vars + 1)\n",
    "            beta[0] = y_mean - np.sum(beta_standard[1:] * y_std * X_mean / X_std)\n",
    "            beta[1:] = beta_standard[1:] * y_std / X_std\n",
    "            \n",
    "            yearly_estimates[i]['intercepts'].append(beta[0])\n",
    "            yearly_estimates[i]['coefs'].append(beta[1:])\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "# Aggregate yearly estimates\n",
    "print(\"\\nAggregating yearly estimates...\")\n",
    "for i in range(n_vars):\n",
    "    if yearly_estimates[i]['coefs']:\n",
    "        # Use median instead of mean for robustness\n",
    "        intercepts[i] = np.median(yearly_estimates[i]['intercepts'])\n",
    "        Gamma[i, :] = np.median(yearly_estimates[i]['coefs'], axis=0)\n",
    "    else:\n",
    "        print(f\"  No valid estimates for equation {i} ({state_vars[i]})\")\n",
    "\n",
    "# Method 2: If year-by-year fails, use pooled estimation with regularization\n",
    "if np.any(np.isnan(Gamma)) or np.all(Gamma == 0):\n",
    "    print(\"\\nUsing pooled estimation with regularization...\")\n",
    "    \n",
    "    from sklearn.linear_model import Ridge\n",
    "    \n",
    "    for i, dep_var in enumerate(state_vars):\n",
    "        # Get all data\n",
    "        y = df_var[dep_var].values\n",
    "        X = df_var[[f'{var}_lag' for var in state_vars]].values\n",
    "        \n",
    "        # Valid data\n",
    "        valid_mask = ~np.isnan(y) & ~np.any(np.isnan(X), axis=1)\n",
    "        \n",
    "        if np.sum(valid_mask) > 100:  # Need reasonable amount of data\n",
    "            y_valid = y[valid_mask]\n",
    "            X_valid = X[valid_mask]\n",
    "            \n",
    "            # Ridge regression with cross-validation for regularization\n",
    "            from sklearn.model_selection import cross_val_score\n",
    "            \n",
    "            alphas = [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "            best_alpha = 0.1\n",
    "            best_score = -np.inf\n",
    "            \n",
    "            for alpha in alphas:\n",
    "                ridge = Ridge(alpha=alpha, fit_intercept=True)\n",
    "                scores = cross_val_score(ridge, X_valid, y_valid, cv=5, \n",
    "                                       scoring='neg_mean_squared_error')\n",
    "                if np.mean(scores) > best_score:\n",
    "                    best_score = np.mean(scores)\n",
    "                    best_alpha = alpha\n",
    "            \n",
    "            # Fit with best alpha\n",
    "            ridge = Ridge(alpha=best_alpha, fit_intercept=True)\n",
    "            ridge.fit(X_valid, y_valid)\n",
    "            \n",
    "            intercepts[i] = ridge.intercept_\n",
    "            Gamma[i, :] = ridge.coef_\n",
    "            print(f\"  {dep_var}: Ridge regression successful (alpha={best_alpha})\")\n",
    "\n",
    "# Ensure stability of VAR\n",
    "print(\"\\nChecking and enforcing VAR stability...\")\n",
    "\n",
    "# Check eigenvalues\n",
    "try:\n",
    "    eigenvalues = np.linalg.eigvals(Gamma)\n",
    "    max_eigenvalue = np.max(np.abs(eigenvalues))\n",
    "    print(f\"Initial max eigenvalue: {max_eigenvalue:.4f}\")\n",
    "    \n",
    "    # If unstable, shrink towards zero\n",
    "    if max_eigenvalue >= 0.99 or np.any(np.isnan(eigenvalues)):\n",
    "        print(\"VAR is unstable or has numerical issues. Applying shrinkage...\")\n",
    "        \n",
    "        # Shrink eigenvalues\n",
    "        shrink_factor = 0.95 / max(max_eigenvalue, 1.0)\n",
    "        Gamma = Gamma * shrink_factor\n",
    "        \n",
    "        # Recheck\n",
    "        eigenvalues = np.linalg.eigvals(Gamma)\n",
    "        max_eigenvalue = np.max(np.abs(eigenvalues))\n",
    "        print(f\"After shrinkage, max eigenvalue: {max_eigenvalue:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Eigenvalue calculation failed: {e}\")\n",
    "    print(\"Using conservative diagonal VAR as fallback...\")\n",
    "    \n",
    "    # Fallback: Use diagonal VAR (each variable depends only on its own lag)\n",
    "    Gamma = np.eye(n_vars) * 0.5  # Conservative persistence\n",
    "\n",
    "# Final validation\n",
    "print(\"\\nFinal Gamma matrix validation:\")\n",
    "print(f\"Contains NaN: {np.any(np.isnan(Gamma))}\")\n",
    "print(f\"Contains Inf: {np.any(np.isinf(Gamma))}\")\n",
    "print(f\"Max absolute value: {np.max(np.abs(Gamma)):.4f}\")\n",
    "\n",
    "# If still have issues, use very conservative values\n",
    "if np.any(np.isnan(Gamma)) or np.any(np.isinf(Gamma)):\n",
    "    print(\"\\nUsing conservative default VAR parameters...\")\n",
    "    Gamma = np.eye(n_vars) * 0.3  # Low persistence\n",
    "    intercepts = np.zeros(n_vars)  # Zero intercepts\n",
    "\n",
    "print(\"\\nGamma matrix (first 5x5):\")\n",
    "print(Gamma[:5, :5])\n",
    "print(f\"\\nIntercepts (first 5): {intercepts[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimating covariance matrix Sigma...\n",
      "Collected 441304 valid residuals from 48 years\n",
      "Total residual matrix shape: (441304, 12)\n",
      "Sigma calculated from 441304 observations\n",
      "\n",
      "Validating Sigma matrix...\n",
      "Sigma shape: (12, 12)\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "\n",
      "Ensuring Sigma is positive definite...\n",
      "\n",
      "Final Sigma matrix check:\n",
      "Contains NaN: False\n",
      "Contains Inf: False\n",
      "Min eigenvalue: 0.000734\n",
      "Max eigenvalue: 0.402323\n",
      "\n",
      "Sigma matrix (first 5x5):\n",
      "[[ 0.01990136  0.00040189  0.00161594 -0.01171618 -0.00575493]\n",
      " [ 0.00040189  0.01020704  0.00166511 -0.00193363 -0.00035582]\n",
      " [ 0.00161594  0.00166511  0.02775113 -0.01779469  0.00742662]\n",
      " [-0.01171618 -0.00193363 -0.01779469  0.07660605  0.00526991]\n",
      " [-0.00575493 -0.00035582  0.00742662  0.00526991  0.12132656]]\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# STEP 5: ESTIMATE COVARIANCE MATRIX (FIXED)\n",
    "# =====================================\n",
    "\n",
    "print(\"\\nEstimating covariance matrix Sigma...\")\n",
    "\n",
    "# Method 1: Calculate residuals year by year and pool them\n",
    "all_residuals = []\n",
    "valid_residual_count = 0\n",
    "\n",
    "for year in sorted(df_var['year'].unique()):\n",
    "    year_data = df_var[df_var['year'] == year]\n",
    "    \n",
    "    if len(year_data) < 10:  # Skip years with too few observations\n",
    "        continue\n",
    "    \n",
    "    # State matrix at t-1 (lagged values)\n",
    "    X_lag = year_data[[f'{var}_lag' for var in state_vars]].values\n",
    "    \n",
    "    # State matrix at t (current values)\n",
    "    X_current = year_data[state_vars].values\n",
    "    \n",
    "    # Check for valid data\n",
    "    valid_mask = ~np.any(np.isnan(X_lag), axis=1) & ~np.any(np.isnan(X_current), axis=1)\n",
    "    \n",
    "    if np.sum(valid_mask) < 5:\n",
    "        continue\n",
    "    \n",
    "    # Filter to valid observations\n",
    "    X_lag_valid = X_lag[valid_mask]\n",
    "    X_current_valid = X_current[valid_mask]\n",
    "    \n",
    "    # Predicted values: X_t = intercepts + X_t-1 * Gamma'\n",
    "    X_pred = intercepts[np.newaxis, :] + X_lag_valid @ Gamma.T\n",
    "    \n",
    "    # Residuals\n",
    "    residuals = X_current_valid - X_pred\n",
    "    \n",
    "    # Store residuals\n",
    "    all_residuals.append(residuals)\n",
    "    valid_residual_count += len(residuals)\n",
    "\n",
    "print(f\"Collected {valid_residual_count} valid residuals from {len(all_residuals)} years\")\n",
    "\n",
    "# Check if we have residuals\n",
    "if all_residuals:\n",
    "    # Stack all residuals\n",
    "    all_residuals_stacked = np.vstack(all_residuals)\n",
    "    print(f\"Total residual matrix shape: {all_residuals_stacked.shape}\")\n",
    "    \n",
    "    # Calculate covariance matrix\n",
    "    # Remove any remaining NaN rows\n",
    "    valid_rows = ~np.any(np.isnan(all_residuals_stacked), axis=1)\n",
    "    clean_residuals = all_residuals_stacked[valid_rows]\n",
    "    \n",
    "    if len(clean_residuals) > n_vars:\n",
    "        # Calculate covariance\n",
    "        Sigma = np.cov(clean_residuals.T)\n",
    "        print(f\"Sigma calculated from {len(clean_residuals)} observations\")\n",
    "    else:\n",
    "        print(\"Not enough valid residuals for covariance calculation\")\n",
    "        Sigma = np.eye(n_vars) * 0.1  # Default diagonal covariance\n",
    "else:\n",
    "    print(\"No valid residuals found!\")\n",
    "    Sigma = np.eye(n_vars) * 0.1  # Default diagonal covariance\n",
    "\n",
    "# Validate Sigma\n",
    "print(\"\\nValidating Sigma matrix...\")\n",
    "print(f\"Sigma shape: {Sigma.shape}\")\n",
    "print(f\"Contains NaN: {np.any(np.isnan(Sigma))}\")\n",
    "print(f\"Contains Inf: {np.any(np.isinf(Sigma))}\")\n",
    "\n",
    "# If Sigma has issues, try alternative estimation\n",
    "if np.any(np.isnan(Sigma)) or np.any(np.isinf(Sigma)):\n",
    "    print(\"\\nSigma has numerical issues. Trying alternative estimation...\")\n",
    "    \n",
    "    # Method 2: Direct calculation equation by equation\n",
    "    Sigma = np.zeros((n_vars, n_vars))\n",
    "    \n",
    "    for i in range(n_vars):\n",
    "        for j in range(i, n_vars):  # Only upper triangle due to symmetry\n",
    "            cov_ij_values = []\n",
    "            \n",
    "            for year in sorted(df_var['year'].unique()):\n",
    "                year_data = df_var[df_var['year'] == year]\n",
    "                \n",
    "                # Get residuals for variables i and j\n",
    "                y_i = year_data[state_vars[i]].values\n",
    "                y_j = year_data[state_vars[j]].values\n",
    "                X_lag = year_data[[f'{var}_lag' for var in state_vars]].values\n",
    "                \n",
    "                # Valid observations\n",
    "                valid_mask = ~np.isnan(y_i) & ~np.isnan(y_j) & ~np.any(np.isnan(X_lag), axis=1)\n",
    "                \n",
    "                if np.sum(valid_mask) < 5:\n",
    "                    continue\n",
    "                \n",
    "                # Predictions\n",
    "                pred_i = intercepts[i] + X_lag[valid_mask] @ Gamma[i, :]\n",
    "                pred_j = intercepts[j] + X_lag[valid_mask] @ Gamma[j, :]\n",
    "                \n",
    "                # Residuals\n",
    "                resid_i = y_i[valid_mask] - pred_i\n",
    "                resid_j = y_j[valid_mask] - pred_j\n",
    "                \n",
    "                # Store covariance for this year\n",
    "                cov_ij_values.append(np.mean(resid_i * resid_j))\n",
    "            \n",
    "            # Average across years\n",
    "            if cov_ij_values:\n",
    "                Sigma[i, j] = np.median(cov_ij_values)  # Use median for robustness\n",
    "                Sigma[j, i] = Sigma[i, j]  # Symmetric\n",
    "            else:\n",
    "                # Default: small positive value on diagonal, zero off-diagonal\n",
    "                Sigma[i, j] = 0.1 if i == j else 0.0\n",
    "                Sigma[j, i] = Sigma[i, j]\n",
    "\n",
    "# Ensure Sigma is positive definite\n",
    "print(\"\\nEnsuring Sigma is positive definite...\")\n",
    "try:\n",
    "    # Check eigenvalues\n",
    "    eigvals = np.linalg.eigvals(Sigma)\n",
    "    min_eigval = np.min(eigvals)\n",
    "    \n",
    "    if min_eigval <= 0 or np.any(np.isnan(eigvals)):\n",
    "        print(f\"Sigma is not positive definite (min eigenvalue: {min_eigval})\")\n",
    "        \n",
    "        # Fix using eigenvalue decomposition\n",
    "        eigvals, eigvecs = np.linalg.eigh(Sigma)\n",
    "        \n",
    "        # Set minimum eigenvalue\n",
    "        min_allowed = 0.001\n",
    "        eigvals = np.maximum(eigvals, min_allowed)\n",
    "        \n",
    "        # Reconstruct\n",
    "        Sigma = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
    "        print(\"Fixed Sigma to be positive definite\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Eigenvalue check failed: {e}\")\n",
    "    # Fallback: diagonal matrix\n",
    "    Sigma = np.diag(np.ones(n_vars) * 0.1)\n",
    "\n",
    "# Final check\n",
    "print(\"\\nFinal Sigma matrix check:\")\n",
    "print(f\"Contains NaN: {np.any(np.isnan(Sigma))}\")\n",
    "print(f\"Contains Inf: {np.any(np.isinf(Sigma))}\")\n",
    "print(f\"Min eigenvalue: {np.min(np.linalg.eigvals(Sigma)):.6f}\")\n",
    "print(f\"Max eigenvalue: {np.max(np.linalg.eigvals(Sigma)):.6f}\")\n",
    "\n",
    "# Display sample of Sigma\n",
    "print(\"\\nSigma matrix (first 5x5):\")\n",
    "print(Sigma[:5, :5])\n",
    "\n",
    "# If still problematic, use very simple diagonal matrix\n",
    "if np.any(np.isnan(Sigma)) or np.any(np.isinf(Sigma)):\n",
    "    print(\"\\nUsing simple diagonal Sigma as last resort...\")\n",
    "    Sigma = np.eye(n_vars) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# STEP 6: ADJUST VAR FOR STABILITY\n",
    "# =====================================\n",
    "\n",
    "# Create steady-state Gamma (diagonal with zeros)\n",
    "Gamma_ss = np.zeros_like(Gamma)\n",
    "\n",
    "# Adjusted Gamma\n",
    "Gamma_adj = theta * Gamma + (1 - theta) * Gamma_ss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# STEP 7: CALCULATE JENSEN'S INEQUALITY ADJUSTMENTS\n",
    "# =====================================\n",
    "\n",
    "# Define selector vectors\n",
    "idx_csprof = state_vars.index('csprof')\n",
    "idx_beg = state_vars.index('beg')\n",
    "\n",
    "# Selector vectors\n",
    "e_csprof = np.zeros(n_vars)\n",
    "e_csprof[idx_csprof] = 1\n",
    "\n",
    "e_beg = np.zeros(n_vars)\n",
    "e_beg[idx_beg] = 1\n",
    "\n",
    "e_po = e_csprof - e_beg  # for payout\n",
    "\n",
    "# Calculate v1(h) and v2(h) for h = 1 to H\n",
    "v1_values = np.zeros(H + 1)\n",
    "v2_values = np.zeros(H + 1)\n",
    "\n",
    "# For v1(h)\n",
    "for h in range(1, H + 1):\n",
    "    if h == 1:\n",
    "        v1_values[h] = 0.5 * e_po @ Sigma @ e_po + e_po @ Sigma @ e_beg\n",
    "    else:\n",
    "        Gamma_power = np.linalg.matrix_power(Gamma_adj, h-1)\n",
    "        v1_values[h] = v1_values[h-1] + 0.5 * e_po @ Gamma_power @ Sigma @ Gamma_power.T @ e_po\n",
    "        \n",
    "        # Calculate F1(h) = sum of Gamma^i from i=0 to h-1\n",
    "        F1_h = np.eye(n_vars)\n",
    "        for i in range(1, h):\n",
    "            F1_h += np.linalg.matrix_power(Gamma_adj, i)\n",
    "        \n",
    "        v1_values[h] += e_po @ Gamma_power @ Sigma @ F1_h.T @ e_beg\n",
    "\n",
    "# For v2(h) - more complex calculation\n",
    "for h in range(1, H + 1):\n",
    "    if h == 1:\n",
    "        v2_values[h] = 0.5 * e_beg @ Sigma @ e_beg\n",
    "    else:\n",
    "        # Need to calculate covariance terms\n",
    "        h_v2_h = (h-1) * v2_values[h-1]\n",
    "        \n",
    "        # Add CovBEg_h,h term\n",
    "        h_v2_h += 0.5 * e_beg @ Sigma @ e_beg\n",
    "        \n",
    "        # Add sum of CovBEg_h-i,h terms\n",
    "        for i in range(1, h):\n",
    "            # Calculate F2 matrix for covariance\n",
    "            F2 = np.zeros((n_vars, n_vars))\n",
    "            for j in range(i):\n",
    "                if h-i+j < h:\n",
    "                    F2 += np.linalg.matrix_power(Gamma_adj, j) @ Sigma @ np.linalg.matrix_power(Gamma_adj, h-i+j).T\n",
    "            \n",
    "            h_v2_h += e_beg @ F2 @ e_beg\n",
    "        \n",
    "        v2_values[h] = h_v2_h / h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# STEP 8: CALCULATE STEADY-STATE VALUES\n",
    "# =====================================\n",
    "idx_bm = state_vars.index('bm')\n",
    "# Steady state: s_bar = (I - Gamma)^(-1) * intercepts\n",
    "I = np.eye(n_vars)\n",
    "s_bar = np.linalg.inv(I - Gamma) @ intercepts\n",
    "\n",
    "# Market-to-book ratio steady state\n",
    "MB_bar = np.exp(-s_bar[idx_bm])  # Since bm = log(BE/ME), so ME/BE = exp(-bm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common discount rate dr = 0.3158\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# STEP 9: SOLVE FOR COMMON DISCOUNT RATE\n",
    "# =====================================\n",
    "\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "def calculate_mb_given_dr(dr):\n",
    "    \"\"\"Calculate implied MB ratio given discount rate dr\"\"\"\n",
    "    mb_sum = 0\n",
    "    \n",
    "    for h in range(1, H + 1):\n",
    "        # Calculate expected payout ratio\n",
    "        Gamma_h = np.linalg.matrix_power(Gamma, h)\n",
    "        \n",
    "        # Expected log ratios\n",
    "        exp_csprof_minus_beg = (e_csprof - e_beg) @ Gamma_h @ s_bar + v1_values[h]\n",
    "        \n",
    "        # Sum of expected BEg from 1 to h\n",
    "        sum_exp_beg = 0\n",
    "        for tau in range(1, h + 1):\n",
    "            Gamma_tau = np.linalg.matrix_power(Gamma, tau)\n",
    "            sum_exp_beg += e_beg @ Gamma_tau @ s_bar\n",
    "        \n",
    "        sum_exp_beg += h * v2_values[h]\n",
    "        \n",
    "        # Expected payout ratio\n",
    "        exp_po_ratio = (np.exp(exp_csprof_minus_beg) - 1) * np.exp(sum_exp_beg)\n",
    "        \n",
    "        # Discount and add to sum\n",
    "        mb_sum += exp_po_ratio * np.exp(-h * dr)\n",
    "    \n",
    "    # Add terminal value\n",
    "    h = H\n",
    "    Gamma_h = np.linalg.matrix_power(Gamma, h)\n",
    "    exp_csprof_minus_beg = (e_csprof - e_beg) @ Gamma_h @ s_bar + v1_values[h]\n",
    "    sum_exp_beg = 0\n",
    "    for tau in range(1, h + 1):\n",
    "        Gamma_tau = np.linalg.matrix_power(Gamma, tau)\n",
    "        sum_exp_beg += e_beg @ Gamma_tau @ s_bar\n",
    "    sum_exp_beg += h * v2_values[h]\n",
    "    \n",
    "    exp_po_ratio_H = (np.exp(exp_csprof_minus_beg) - 1) * np.exp(sum_exp_beg)\n",
    "    \n",
    "    # Terminal value calculation\n",
    "    beg_bar = e_beg @ s_bar\n",
    "    terminal_growth = beg_bar + v2_values[H] - dr\n",
    "    \n",
    "    if terminal_growth < 0:  # Ensure convergence\n",
    "        pv_cf_ratio = np.exp(terminal_growth) / (1 - np.exp(terminal_growth))\n",
    "        mb_sum += exp_po_ratio_H * np.exp(-H * dr) * pv_cf_ratio\n",
    "    \n",
    "    return mb_sum\n",
    "\n",
    "# Solve for dr such that calculated MB equals steady-state MB\n",
    "dr_initial_guess = 0.1  # 10% initial guess\n",
    "\n",
    "# Define the equation to solve\n",
    "def mb_equation(dr):\n",
    "    return calculate_mb_given_dr(dr) - MB_bar\n",
    "\n",
    "# Solve for dr\n",
    "dr_solution = fsolve(mb_equation, dr_initial_guess)[0]\n",
    "print(f\"Common discount rate dr = {dr_solution:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of state variables:\n",
      "bm: float64\n",
      "poy: float64\n",
      "yy: float64\n",
      "beg: float64\n",
      "ag: float64\n",
      "yg: float64\n",
      "csprof: float64\n",
      "roe: float64\n",
      "gprof: float64\n",
      "mlev: float64\n",
      "blev: float64\n",
      "cash: float64\n",
      "\n",
      "Missing values after conversion:\n",
      "bm            0\n",
      "poy           0\n",
      "yy            0\n",
      "beg           0\n",
      "ag        18044\n",
      "yg        18293\n",
      "csprof        0\n",
      "roe       10952\n",
      "gprof       791\n",
      "mlev          0\n",
      "blev          0\n",
      "cash          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check data types of state variables\n",
    "print(\"Data types of state variables:\")\n",
    "for var in state_vars:\n",
    "    print(f\"{var}: {df_annual[var].dtype}\")\n",
    "\n",
    "# Convert to numeric if needed\n",
    "for var in state_vars:\n",
    "    df_annual[var] = pd.to_numeric(df_annual[var], errors='coerce')\n",
    "\n",
    "# Check for any remaining non-numeric values\n",
    "print(\"\\nMissing values after conversion:\")\n",
    "print(df_annual[state_vars].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data quality...\n",
      "Total observations: 70141\n",
      "Observations with complete state variables: 41455\n",
      "\n",
      "Pre-computing matrix powers (this speeds up calculation significantly)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Gamma powers: 100%|██████████| 99/99 [00:00<00:00, 199249.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-computing cumulative Gamma sums...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing cumulative sums: 100%|██████████| 100/100 [00:00<00:00, 658446.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating FE for 70141 observations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating FE: 100%|██████████| 70141/70141 [00:00<00:00, 247095.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying bounds to FE values...\n",
      "\n",
      "==================================================\n",
      "FE CALCULATION COMPLETE\n",
      "==================================================\n",
      "Total observations: 70141\n",
      "Valid FE values calculated: 41452\n",
      "Percentage with valid FE: 59.1%\n",
      "\n",
      "FE statistics:\n",
      "count    4.145200e+04\n",
      "mean    -8.953116e+07\n",
      "std      3.680785e+09\n",
      "min     -3.965668e+11\n",
      "25%     -6.778961e+06\n",
      "50%      4.343578e+06\n",
      "75%      5.132976e+07\n",
      "max      1.286701e+11\n",
      "Name: FE, dtype: float64\n",
      "\n",
      "FE/ME ratio statistics:\n",
      "count    41452.000000\n",
      "mean         0.273345\n",
      "std          5.422839\n",
      "min         -2.089470\n",
      "25%         -0.024593\n",
      "50%          0.039558\n",
      "75%          0.126129\n",
      "max        647.174446\n",
      "dtype: float64\n",
      "\n",
      "Saving results...\n",
      "Results saved!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import fsolve\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================\n",
    "# STEP 10: CALCULATE FE FOR EACH FIRM (OPTIMIZED WITH PROGRESS BAR)\n",
    "# =====================================\n",
    "\n",
    "# First, check data quality\n",
    "print(\"Checking data quality...\")\n",
    "print(f\"Total observations: {len(df_annual)}\")\n",
    "print(f\"Observations with complete state variables: {df_annual[state_vars].notna().all(axis=1).sum()}\")\n",
    "\n",
    "# Ensure all state variables are numeric\n",
    "for var in state_vars:\n",
    "    df_annual[var] = pd.to_numeric(df_annual[var], errors='coerce')\n",
    "\n",
    "# Pre-compute matrix powers to avoid redundant calculations\n",
    "print(\"\\nPre-computing matrix powers (this speeds up calculation significantly)...\")\n",
    "H_practical = 100  # Reduce from 1000 to 100 for practical computation\n",
    "\n",
    "# Pre-compute all Gamma powers\n",
    "Gamma_powers = {}\n",
    "Gamma_powers[0] = np.eye(n_vars)\n",
    "Gamma_powers[1] = Gamma\n",
    "\n",
    "for h in tqdm(range(2, H_practical + 1), desc=\"Computing Gamma powers\"):\n",
    "    Gamma_powers[h] = Gamma_powers[h-1] @ Gamma\n",
    "\n",
    "# Pre-compute cumulative sums of Gamma powers for BEg calculation\n",
    "print(\"\\nPre-computing cumulative Gamma sums...\")\n",
    "Gamma_cumsum = {}\n",
    "Gamma_cumsum[0] = np.zeros((n_vars, n_vars))\n",
    "\n",
    "for h in tqdm(range(1, H_practical + 1), desc=\"Computing cumulative sums\"):\n",
    "    Gamma_cumsum[h] = Gamma_cumsum[h-1] + Gamma_powers[h]\n",
    "\n",
    "# Vectorized FE calculation with batching\n",
    "print(f\"\\nCalculating FE for {len(df_annual)} observations...\")\n",
    "\n",
    "# Function to calculate FE for a batch of firms\n",
    "def calculate_fe_batch(batch_df, batch_idx, total_batches):\n",
    "    \"\"\"Calculate FE for a batch of firms efficiently\"\"\"\n",
    "    batch_size = len(batch_df)\n",
    "    fe_results = np.full(batch_size, np.nan)\n",
    "    \n",
    "    # Get state matrix for batch\n",
    "    state_matrix = batch_df[state_vars].values\n",
    "    be_values = batch_df['BE'].values\n",
    "    \n",
    "    # Find valid observations (no NaN in state variables and positive BE)\n",
    "    valid_mask = (~np.any(np.isnan(state_matrix), axis=1)) & (be_values > 0)\n",
    "    \n",
    "    if not np.any(valid_mask):\n",
    "        return fe_results\n",
    "    \n",
    "    # Process only valid observations\n",
    "    valid_states = state_matrix[valid_mask]\n",
    "    valid_be = be_values[valid_mask]\n",
    "    n_valid = valid_states.shape[0]\n",
    "    \n",
    "    # Initialize FE/BE ratios\n",
    "    fe_be_ratios = np.zeros(n_valid)\n",
    "    \n",
    "    # Calculate for each h (vectorized across firms)\n",
    "    for h in range(1, H_practical + 1):\n",
    "        # Vectorized calculation for all valid firms at once\n",
    "        # exp_csprof_minus_beg for all firms\n",
    "        exp_terms = valid_states @ Gamma_powers[h].T @ (e_csprof - e_beg) + v1_values[h]\n",
    "        \n",
    "        # Sum of expected BEg for all firms\n",
    "        sum_beg_terms = valid_states @ Gamma_cumsum[h].T @ e_beg + h * v2_values[h]\n",
    "        \n",
    "        # Clip to prevent overflow\n",
    "        exp_terms = np.clip(exp_terms, -100, 100)\n",
    "        sum_beg_terms = np.clip(sum_beg_terms, -100, 100)\n",
    "        \n",
    "        # Expected payout ratios\n",
    "        exp_po_ratios = (np.exp(exp_terms) - 1) * np.exp(sum_beg_terms)\n",
    "        \n",
    "        # Discount and add\n",
    "        contributions = exp_po_ratios * np.exp(-h * dr_solution)\n",
    "        fe_be_ratios += contributions\n",
    "        \n",
    "        # Early stopping if contributions become negligible\n",
    "        if h > 20 and np.max(np.abs(contributions)) < 1e-10:\n",
    "            break\n",
    "    \n",
    "    # Add simple terminal value\n",
    "    if h == H_practical:\n",
    "        # Use Gordon growth model approximation for terminal value\n",
    "        terminal_growth = valid_states @ e_beg + v2_values[H_practical] - dr_solution\n",
    "        \n",
    "        # Only calculate terminal value where it converges\n",
    "        converging_mask = terminal_growth < -0.01\n",
    "        if np.any(converging_mask):\n",
    "            terminal_pv_ratios = np.zeros_like(terminal_growth)\n",
    "            terminal_pv_ratios[converging_mask] = (\n",
    "                np.exp(terminal_growth[converging_mask]) / \n",
    "                (1 - np.exp(terminal_growth[converging_mask]))\n",
    "            )\n",
    "            \n",
    "            # Last period's contribution times terminal PV ratio\n",
    "            last_contributions = exp_po_ratios * np.exp(-H_practical * dr_solution)\n",
    "            fe_be_ratios += last_contributions * terminal_pv_ratios\n",
    "    \n",
    "    # Calculate FE values\n",
    "    fe_values = valid_be * fe_be_ratios\n",
    "    \n",
    "    # Place results back in correct positions\n",
    "    fe_results[valid_mask] = fe_values\n",
    "    \n",
    "    return fe_results\n",
    "\n",
    "# Process in batches with progress bar\n",
    "batch_size = 1000  # Process 1000 firms at a time\n",
    "n_batches = (len(df_annual) + batch_size - 1) // batch_size\n",
    "\n",
    "all_fe_results = []\n",
    "\n",
    "# Main progress bar for batches\n",
    "with tqdm(total=len(df_annual), desc=\"Calculating FE\") as pbar:\n",
    "    for batch_idx in range(n_batches):\n",
    "        # Get batch\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(df_annual))\n",
    "        batch_df = df_annual.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Calculate FE for batch\n",
    "        batch_results = calculate_fe_batch(batch_df, batch_idx, n_batches)\n",
    "        all_fe_results.extend(batch_results)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.update(len(batch_df))\n",
    "\n",
    "# Add results to dataframe\n",
    "df_annual['FE'] = all_fe_results\n",
    "\n",
    "# Apply bounds as mentioned in the paper\n",
    "print(\"\\nApplying bounds to FE values...\")\n",
    "df_annual['FE_bounded'] = df_annual['FE'].copy()\n",
    "lower_bound = df_annual[['ME', 'BE']].max(axis=1) / 100\n",
    "upper_bound = df_annual[['ME', 'BE']].min(axis=1) * 100\n",
    "\n",
    "# Only apply bounds where FE is not NaN\n",
    "valid_fe_mask = df_annual['FE'].notna()\n",
    "df_annual.loc[valid_fe_mask, 'FE_bounded'] = df_annual.loc[valid_fe_mask, 'FE'].clip(\n",
    "    lower=lower_bound[valid_fe_mask], \n",
    "    upper=upper_bound[valid_fe_mask]\n",
    ")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FE CALCULATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total observations: {len(df_annual)}\")\n",
    "print(f\"Valid FE values calculated: {df_annual['FE'].notna().sum()}\")\n",
    "print(f\"Percentage with valid FE: {100 * df_annual['FE'].notna().sum() / len(df_annual):.1f}%\")\n",
    "print(f\"\\nFE statistics:\")\n",
    "print(df_annual['FE'].describe())\n",
    "print(f\"\\nFE/ME ratio statistics:\")\n",
    "print((df_annual['FE'] / df_annual['ME']).describe())\n",
    "\n",
    "# Optional: Save intermediate results to avoid re-computation\n",
    "print(\"\\nSaving results...\")\n",
    "df_annual.to_pickle('df_annual_with_fe.pkl')  # Save as pickle to preserve data types\n",
    "# Or save as CSV: df_annual.to_csv('df_annual_with_fe.csv', index=False)\n",
    "print(\"Results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating FE for 70141 firm-year observations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating FE: 100%|██████████| 70141/70141 [48:15<00:00, 24.23firms/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FE calculation complete!\n",
      "Valid FE values: 41452 out of 70141\n",
      "Success rate: 59.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# STEP 10: CALCULATE FE FOR EACH FIRM (WITH PROGRESS BAR)\n",
    "# =====================================\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Now calculate FE for each firm-year observation\n",
    "print(f\"\\nCalculating FE for {len(df_annual)} firm-year observations...\")\n",
    "\n",
    "fe_results = []\n",
    "\n",
    "# Create progress bar\n",
    "with tqdm(total=len(df_annual), desc=\"Calculating FE\", unit=\"firms\") as pbar:\n",
    "    for idx, row in df_annual.iterrows():\n",
    "        # Get current state vector\n",
    "        try:\n",
    "            # Try to convert to numeric values\n",
    "            s_current = pd.to_numeric(row[state_vars], errors='coerce').values\n",
    "            \n",
    "            # Skip if any state variable is missing or non-numeric\n",
    "            if np.any(np.isnan(s_current)):\n",
    "                fe_results.append(np.nan)\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            # If conversion fails, append NaN and continue\n",
    "            fe_results.append(np.nan)\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        \n",
    "        # Calculate FE/BE ratio\n",
    "        fe_be_ratio = 0\n",
    "        \n",
    "        for h in range(1, H + 1):\n",
    "            # Calculate expected payout ratio\n",
    "            Gamma_h = np.linalg.matrix_power(Gamma, h)\n",
    "            \n",
    "            # Expected log ratios\n",
    "            exp_csprof_minus_beg = (e_csprof - e_beg) @ Gamma_h @ s_current + v1_values[h]\n",
    "            \n",
    "            # Sum of expected BEg from 1 to h\n",
    "            sum_exp_beg = 0\n",
    "            for tau in range(1, h + 1):\n",
    "                Gamma_tau = np.linalg.matrix_power(Gamma, tau)\n",
    "                sum_exp_beg += e_beg @ Gamma_tau @ s_current\n",
    "            \n",
    "            sum_exp_beg += h * v2_values[h]\n",
    "            \n",
    "            # Expected payout ratio\n",
    "            exp_po_ratio = (np.exp(exp_csprof_minus_beg) - 1) * np.exp(sum_exp_beg)\n",
    "            \n",
    "            # Discount and add to sum\n",
    "            fe_be_ratio += exp_po_ratio * np.exp(-h * dr_solution)\n",
    "        \n",
    "        # Add terminal value\n",
    "        h = H\n",
    "        Gamma_h = np.linalg.matrix_power(Gamma, h)\n",
    "        exp_csprof_minus_beg = (e_csprof - e_beg) @ Gamma_h @ s_current + v1_values[h]\n",
    "        sum_exp_beg = 0\n",
    "        for tau in range(1, h + 1):\n",
    "            Gamma_tau = np.linalg.matrix_power(Gamma, tau)\n",
    "            sum_exp_beg += e_beg @ Gamma_tau @ s_current\n",
    "        sum_exp_beg += h * v2_values[h]\n",
    "        \n",
    "        exp_po_ratio_H = (np.exp(exp_csprof_minus_beg) - 1) * np.exp(sum_exp_beg)\n",
    "        \n",
    "        # Terminal value\n",
    "        beg_current = e_beg @ s_current\n",
    "        terminal_growth = beg_current + v2_values[H] - dr_solution\n",
    "        \n",
    "        if terminal_growth < 0:\n",
    "            pv_cf_ratio = np.exp(terminal_growth) / (1 - np.exp(terminal_growth))\n",
    "            fe_be_ratio += exp_po_ratio_H * np.exp(-H * dr_solution) * pv_cf_ratio\n",
    "        \n",
    "        # Calculate FE\n",
    "        FE = row['BE'] * fe_be_ratio\n",
    "        fe_results.append(FE)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.update(1)\n",
    "        \n",
    "# Add FE to dataframe\n",
    "df_annual['FE'] = fe_results\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nFE calculation complete!\")\n",
    "print(f\"Valid FE values: {df_annual['FE'].notna().sum()} out of {len(df_annual)}\")\n",
    "print(f\"Success rate: {100 * df_annual['FE'].notna().sum() / len(df_annual):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n",
      "Number of firms with FE calculated: 41452\n",
      "\n",
      "FE/ME ratio statistics:\n",
      "count    41452.000000\n",
      "mean        -2.969663\n",
      "std          1.390754\n",
      "min         -5.658329\n",
      "25%         -4.162178\n",
      "50%         -3.194198\n",
      "75%         -2.073284\n",
      "max          4.605170\n",
      "Name: fm_bounded, dtype: float64\n",
      "\n",
      "BE/FE ratio statistics:\n",
      "count    41452.000000\n",
      "mean         2.424840\n",
      "std          2.107367\n",
      "min         -4.605170\n",
      "25%          1.022609\n",
      "50%          2.664414\n",
      "75%          4.605170\n",
      "max          4.605170\n",
      "Name: bf_bounded, dtype: float64\n",
      "\n",
      "Verification that bm = fm + bf:\n",
      "Mean absolute difference: 1.562950\n",
      "\n",
      "Merging annual FE calculations back to monthly data...\n",
      "Merged FE data to 41452 monthly observations\n",
      "\n",
      "Final data structure:\n",
      "df_annual shape: (70141, 106) (annual data with FE)\n",
      "df shape: (770998, 103) (monthly data with FE merged)\n",
      "\n",
      "Saving results...\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# STEP 11: CALCULATE KEY RATIOS\n",
    "# =====================================\n",
    "\n",
    "# Calculate fm = log(FE/ME)\n",
    "df_annual['fm'] = np.log(df_annual['FE'] / df_annual['ME'])\n",
    "\n",
    "# Calculate bf = log(BE/FE)\n",
    "df_annual['bf'] = np.log(df_annual['BE'] / df_annual['FE'])\n",
    "\n",
    "# Apply bounds to handle outliers (as mentioned in the paper)\n",
    "# Bound FE at (1/100) * max(ME, BE) from below and 100 * min(ME, BE) from above\n",
    "df_annual['FE_bounded'] = df_annual['FE'].copy()\n",
    "lower_bound = df_annual[['ME', 'BE']].max(axis=1) / 100\n",
    "upper_bound = df_annual[['ME', 'BE']].min(axis=1) * 100\n",
    "df_annual['FE_bounded'] = df_annual['FE_bounded'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# Recalculate ratios with bounded FE\n",
    "df_annual['fm_bounded'] = np.log(df_annual['FE_bounded'] / df_annual['ME'])\n",
    "df_annual['bf_bounded'] = np.log(df_annual['BE'] / df_annual['FE_bounded'])\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Number of firms with FE calculated: {df_annual['FE'].notna().sum()}\")\n",
    "print(f\"\\nFE/ME ratio statistics:\")\n",
    "print(df_annual['fm_bounded'].describe())\n",
    "print(f\"\\nBE/FE ratio statistics:\")\n",
    "print(df_annual['bf_bounded'].describe())\n",
    "\n",
    "# Verify the identity: bm = fm + bf\n",
    "df_annual['bm_check'] = df_annual['fm_bounded'] + df_annual['bf_bounded']\n",
    "print(f\"\\nVerification that bm = fm + bf:\")\n",
    "print(f\"Mean absolute difference: {np.abs(df_annual['bm'] - df_annual['bm_check']).mean():.6f}\")\n",
    "\n",
    "# =====================================\n",
    "# STEP 12: MERGE BACK TO MONTHLY DATA (OPTIONAL)\n",
    "# =====================================\n",
    "\n",
    "# If you want to have FE values for all months in the original df\n",
    "if 'FE' not in df.columns:\n",
    "    print(\"\\nMerging annual FE calculations back to monthly data...\")\n",
    "    \n",
    "    # Select columns to merge\n",
    "    merge_cols = ['permno', 'date', 'FE', 'FE_bounded', 'fm', 'bf', 'fm_bounded', 'bf_bounded']\n",
    "    \n",
    "    # Merge\n",
    "    df = df.merge(\n",
    "        df_annual[merge_cols], \n",
    "        on=['permno', 'date'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"Merged FE data to {df['FE'].notna().sum()} monthly observations\")\n",
    "\n",
    "# Show final data structure\n",
    "print(\"\\nFinal data structure:\")\n",
    "print(f\"df_annual shape: {df_annual.shape} (annual data with FE)\")\n",
    "print(f\"df shape: {df.shape} (monthly data with FE merged)\")\n",
    "\n",
    "# Save results\n",
    "print(\"\\nSaving results...\")\n",
    "df_annual.to_csv('fundamental_equity_annual.csv', index=False)\n",
    "# Or as pickle to preserve data types:\n",
    "# df_annual.to_pickle('fundamental_equity_annual.pkl')\n",
    "\n",
    "print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamental_equity = df_annual.pivot(index='date', columns='permno', values='FE')\n",
    "fm = df.pivot(index='date', columns='permno', values='fm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(df_annual, 'df_annual.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = pd.read_pickle('Data/price.pkl')\n",
    "bookvalue = pd.read_pickle('Data/bookvalue.pkl')\n",
    "dividends = pd.read_pickle('Data/dividends.pkl')\n",
    "shrout = pd.read_pickle('Data/shrout.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CREATING FINAL DATAFRAMES ===\n",
      "FEHLER: Keine gültigen Daten gefunden!\n",
      "\n",
      "Ergebnisse gespeichert in 'long_short_returns_clean.csv'\n"
     ]
    }
   ],
   "source": [
    "# VOLLSTÄNDIGE RENDITEBERECHNUNG ÜBER PREIS-DIVIDENDEN-FORMEL\n",
    "# Formel: R_t = (P_t + D_t) / P_{t-1} - 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialisierung der Listen für Equal Weight Berechnung\n",
    "monthly_returns_long_ew = []\n",
    "monthly_returns_short_ew = []\n",
    "long_short_returns_clean = []\n",
    "\n",
    "# Tracking für Portfolios über die Zeit\n",
    "portfolio_tracking = []\n",
    "\n",
    "for year in sorted(set(date.year for date in fm.index)):\n",
    "    \n",
    "    date = pd.Timestamp(f\"{year}-06\")\n",
    "    date_dec = pd.Timestamp(f\"{year-1}-12\")\n",
    "    \n",
    "    if date_dec not in fm.index:\n",
    "        continue\n",
    "    if year == 2018:  # Daten enden 2018\n",
    "        continue\n",
    "    \n",
    "    # Zeitrahmen: Juli des aktuellen Jahres bis Juni des folgenden Jahres\n",
    "    month_range = pd.date_range(start=date + pd.DateOffset(months=1), periods=12, freq=\"ME\")\n",
    "    \n",
    "    print(f\"Processing year {year}...\")\n",
    "    \n",
    "    # STOCK SELECTION (gleich wie vorher)\n",
    "    clean_row = fm.loc[date_dec].dropna().sort_values(ascending=True)\n",
    "    n = len(clean_row)\n",
    "    if n == 0:\n",
    "        continue\n",
    "        \n",
    "    k = int(n * 0.3)\n",
    "    low_bm_permnos = clean_row.iloc[:k].index.tolist()   # Niedrige BM (Short)\n",
    "    high_bm_permnos = clean_row.iloc[-k:].index.tolist() # Hohe BM (Long)\n",
    "    \n",
    "    # Speichere Portfolio-Zusammensetzung\n",
    "    portfolio_tracking.append({\n",
    "        'year': year,\n",
    "        'rebalance_date': date,\n",
    "        'high_bm_stocks': len(high_bm_permnos),\n",
    "        'low_bm_stocks': len(low_bm_permnos)\n",
    "    })\n",
    "    \n",
    "    # MONATLICHE RENDITEBERECHNUNG über Preis-Dividenden-Formel\n",
    "    for i, current_month in enumerate(month_range):\n",
    "        \n",
    "        current_date = current_month\n",
    "        \n",
    "        # Bestimme vorherigen Monat\n",
    "        if i == 0:\n",
    "            # Erster Monat: verwende Rebalancing-Datum als Basis\n",
    "            previous_date = date\n",
    "        else:\n",
    "            previous_date = month_range[i-1]\n",
    "        \n",
    "        print(f\"  Calculating returns for {current_date.strftime('%Y-%m')}...\")\n",
    "        \n",
    "        # === LONG PORTFOLIO (High BM) ===\n",
    "        individual_returns_long = []\n",
    "        \n",
    "        for permno in high_bm_permnos:\n",
    "            try:\n",
    "                # Aktuelle Preise und Dividenden\n",
    "                if current_date in price.index and permno in price.columns:\n",
    "                    P_t = price.loc[current_date, permno]\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if previous_date in price.index and permno in price.columns:\n",
    "                    P_t_minus_1 = price.loc[previous_date, permno]\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Dividenden für den aktuellen Monat\n",
    "                if current_date in dividends.index and permno in dividends.columns:\n",
    "                    D_t = dividends.loc[current_date, permno]\n",
    "                    if pd.isna(D_t):\n",
    "                        D_t = 0\n",
    "                else:\n",
    "                    D_t = 0\n",
    "                \n",
    "                # Prüfe auf gültige Werte\n",
    "                if pd.isna(P_t) or pd.isna(P_t_minus_1) or P_t_minus_1 <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # RENDITEBERECHNUNG: R = (P_t + D_t) / P_{t-1} - 1\n",
    "                stock_return = (P_t + D_t) / P_t_minus_1 - 1\n",
    "                \n",
    "                # Sanity Check: Extreme Renditen ausschließen\n",
    "                if -0.95 <= stock_return <= 5.0:  # Zwischen -95% und +500%\n",
    "                    individual_returns_long.append(stock_return)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # === SHORT PORTFOLIO (Low BM) ===\n",
    "        individual_returns_short = []\n",
    "        \n",
    "        for permno in low_bm_permnos:\n",
    "            try:\n",
    "                # Aktuelle Preise und Dividenden\n",
    "                if current_date in price.index and permno in price.columns:\n",
    "                    P_t = price.loc[current_date, permno]\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if previous_date in price.index and permno in price.columns:\n",
    "                    P_t_minus_1 = price.loc[previous_date, permno]\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Dividenden für den aktuellen Monat\n",
    "                if current_date in dividends.index and permno in dividends.columns:\n",
    "                    D_t = dividends.loc[current_date, permno]\n",
    "                    if pd.isna(D_t):\n",
    "                        D_t = 0\n",
    "                else:\n",
    "                    D_t = 0\n",
    "                \n",
    "                # Prüfe auf gültige Werte\n",
    "                if pd.isna(P_t) or pd.isna(P_t_minus_1) or P_t_minus_1 <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # RENDITEBERECHNUNG: R = (P_t + D_t) / P_{t-1} - 1\n",
    "                stock_return = (P_t + D_t) / P_t_minus_1 - 1\n",
    "                \n",
    "                # Sanity Check: Extreme Renditen ausschließen\n",
    "                if -0.95 <= stock_return <= 5.0:  # Zwischen -95% und +500%\n",
    "                    individual_returns_short.append(stock_return)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # EQUAL WEIGHT Portfolio-Renditen berechnen\n",
    "        if len(individual_returns_long) > 0:\n",
    "            portfolio_return_long = np.mean(individual_returns_long)\n",
    "            monthly_returns_long_ew.append({\n",
    "                'date': current_date,\n",
    "                'return': portfolio_return_long,\n",
    "                'n_stocks': len(individual_returns_long)\n",
    "            })\n",
    "        else:\n",
    "            portfolio_return_long = np.nan\n",
    "            \n",
    "        if len(individual_returns_short) > 0:\n",
    "            portfolio_return_short = np.mean(individual_returns_short)\n",
    "            monthly_returns_short_ew.append({\n",
    "                'date': current_date,\n",
    "                'return': portfolio_return_short,\n",
    "                'n_stocks': len(individual_returns_short)\n",
    "            })\n",
    "        else:\n",
    "            portfolio_return_short = np.nan\n",
    "        \n",
    "        # LONG-SHORT SPREAD berechnen\n",
    "        if not pd.isna(portfolio_return_long) and not pd.isna(portfolio_return_short):\n",
    "            long_short_spread = portfolio_return_long - portfolio_return_short\n",
    "            long_short_returns_clean.append({\n",
    "                'date': current_date,\n",
    "                'long_return': portfolio_return_long,\n",
    "                'short_return': portfolio_return_short,\n",
    "                'long_short_return': long_short_spread,\n",
    "                'long_stocks': len(individual_returns_long),\n",
    "                'short_stocks': len(individual_returns_short)\n",
    "            })\n",
    "            \n",
    "            print(f\"    Long: {portfolio_return_long:.4f} ({len(individual_returns_long)} stocks)\")\n",
    "            print(f\"    Short: {portfolio_return_short:.4f} ({len(individual_returns_short)} stocks)\")\n",
    "            print(f\"    Spread: {long_short_spread:.4f}\")\n",
    "\n",
    "# ERGEBNISSE ZUSAMMENFASSEN\n",
    "print(\"\\n=== CREATING FINAL DATAFRAMES ===\")\n",
    "\n",
    "# Long-Short Returns DataFrame\n",
    "df_ls_clean = pd.DataFrame(long_short_returns_clean)\n",
    "if not df_ls_clean.empty:\n",
    "    df_ls_clean.set_index('date', inplace=True)\n",
    "    df_ls_clean['cumulative_return'] = (1 + df_ls_clean['long_short_return']).cumprod() - 1\n",
    "\n",
    "# Portfolio Tracking\n",
    "df_portfolio = pd.DataFrame(portfolio_tracking)\n",
    "\n",
    "# STATISTIKEN und PLOTS\n",
    "if not df_ls_clean.empty:\n",
    "    print(\"\\n=== FINAL STATISTICS ===\")\n",
    "    print(f\"Zeitraum: {df_ls_clean.index.min()} bis {df_ls_clean.index.max()}\")\n",
    "    print(f\"Anzahl Monate: {len(df_ls_clean)}\")\n",
    "    print(f\"Durchschnittliche monatliche Rendite: {df_ls_clean['long_short_return'].mean():.4f}\")\n",
    "    print(f\"Annualisierte Rendite: {df_ls_clean['long_short_return'].mean() * 12:.4f}\")\n",
    "    print(f\"Volatilität (monatlich): {df_ls_clean['long_short_return'].std():.4f}\")\n",
    "    print(f\"Volatilität (annualisiert): {df_ls_clean['long_short_return'].std() * np.sqrt(12):.4f}\")\n",
    "    print(f\"Sharpe Ratio: {(df_ls_clean['long_short_return'].mean() * 12) / (df_ls_clean['long_short_return'].std() * np.sqrt(12)):.4f}\")\n",
    "    print(f\"Gesamtrendite: {df_ls_clean['cumulative_return'].iloc[-1]:.4f}\")\n",
    "    print(f\"Durchschnittliche Anzahl Long-Aktien: {df_ls_clean['long_stocks'].mean():.1f}\")\n",
    "    print(f\"Durchschnittliche Anzahl Short-Aktien: {df_ls_clean['short_stocks'].mean():.1f}\")\n",
    "    \n",
    "    # PLOTS\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Kumulative Rendite\n",
    "    df_ls_clean['cumulative_return'].plot(ax=ax1, title=\"Kumulative Long-Short Rendite (Preis-Dividenden-Formel)\")\n",
    "    ax1.set_ylabel(\"Kumulative Rendite\")\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # 2. Monatliche Renditen\n",
    "    df_ls_clean['long_short_return'].plot(ax=ax2, title=\"Monatliche Long-Short Renditen\")\n",
    "    ax2.set_ylabel(\"Monatliche Rendite\")\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # 3. Long vs Short Performance\n",
    "    df_ls_clean[['long_return', 'short_return']].plot(ax=ax3, title=\"Long vs Short Portfolio Returns\")\n",
    "    ax3.set_ylabel(\"Monatliche Rendite\")\n",
    "    ax3.legend(['Long Portfolio', 'Short Portfolio'])\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    # 4. Anzahl Aktien im Portfolio\n",
    "    df_ls_clean[['long_stocks', 'short_stocks']].plot(ax=ax4, title=\"Anzahl Aktien im Portfolio\")\n",
    "    ax4.set_ylabel(\"Anzahl Aktien\")\n",
    "    ax4.legend(['Long Portfolio', 'Short Portfolio'])\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Zusätzliche Diagnose\n",
    "    print(\"\\n=== DIAGNOSE ===\")\n",
    "    print(\"Letzte 10 Renditen:\")\n",
    "    print(df_ls_clean[['long_return', 'short_return', 'long_short_return']].tail(10))\n",
    "    \n",
    "    extreme_returns = df_ls_clean[(df_ls_clean['long_short_return'].abs() > 0.2)]\n",
    "    print(f\"\\nAnzahl extremer Renditen (>20%): {len(extreme_returns)}\")\n",
    "    if len(extreme_returns) > 0:\n",
    "        print(\"Extreme Renditen:\")\n",
    "        print(extreme_returns[['long_return', 'short_return', 'long_short_return']])\n",
    "\n",
    "else:\n",
    "    print(\"FEHLER: Keine gültigen Daten gefunden!\")\n",
    "\n",
    "# Speichere Ergebnisse für weitere Analyse\n",
    "d#f_ls_clean.to_csv('long_short_returns_clean.csv')\n",
    "print(f\"\\nErgebnisse gespeichert in 'long_short_returns_clean.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_long_ts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
